import streamlit as st
from langchain.embeddings import OllamaEmbeddings
from langchain.vectorstores import SupabaseVectorStore
from langchain.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.schema import SystemMessage
from supabase import create_client
import os

# Configura√ß√µes iniciais
SUPABASE_URL = "https://<sua-instancia>.supabase.co"
SUPABASE_KEY = "<sua-chave-secreta>"

# Criar cliente Supabase
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# Streamlit - t√≠tulo
st.set_page_config(page_title="Chat RAG com Supabase", layout="wide")
st.title("üí¨ Chat RAG com LLaMA + Supabase")

# Hist√≥rico de chat
if "history" not in st.session_state:
    st.session_state.history = []

# Entrada do usu√°rio
query = st.chat_input("Digite sua pergunta...")

# Embeddings + Chat model (usando Ollama)
embedding = OllamaEmbeddings(model="llama3:instruct")
llm = ChatOllama(model="llama3:instruct")

# Conex√£o com o vetor store (Supabase)
vectorstore = SupabaseVectorStore(
    supabase_client=supabase,
    embedding=embedding,
    table_name="documents",
    query_name="match_documents"  # (crie uma RPC no Supabase se necess√°rio)
)

# RAG: Chat com recupera√ß√£o
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    return_source_documents=True
)

# Processar pergunta
if query:
    with st.spinner("üîç Buscando resposta..."):
        result = qa_chain({"query": query})
        resposta = result["result"]

        # Mostrar no chat
        st.session_state.history.append(("Voc√™", query))
        st.session_state.history.append(("Agente", resposta))

# Mostrar hist√≥rico
for autor, msg in st.session_state.history:
    with st.chat_message(autor):
        st.markdown(msg)
